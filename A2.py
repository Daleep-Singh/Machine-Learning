import bonnerlib2D as bl2dimport picklewith open('cluster_data.pickle','rb') as file:    dataTrain,dataTest = pickle.load(file)Xtrain,Ttrain = dataTrainXtest,Ttest = dataTestimport sklearn.linear_model as linimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisfrom sklearn.naive_bayes import GaussianNBfrom sklearn.neural_network import MLPClassifier#---------------------------------#q1a#---------------------------------print("Q1 a)\n")clf = lin.LogisticRegression(multi_class='multinomial',solver='lbfgs')clf.fit(Xtrain,Ttrain)Xtrain_score = clf.score(Xtrain,Ttrain)print("Xtrain_score: {0}".format(clf.score(Xtrain,Ttrain)))Xtest_score = clf.score(Xtest,Ttest)print("Xtest_score: {0}".format(Xtest_score))#---------------------------------#q1b#---------------------------------bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(clf.fit(Xtrain,Ttrain))bl2d.plt.suptitle("Question 1(b): decision boundaries for linear classification")#---------------------------------#q1e#---------------------------------print("\nQ1 e)\n")def predict(X,W,b=0):    ''' |X| = K*D        |W| = K*D        |W^T| = D*K        |XW^T| = K*K        |b| = K        '''            XW_T_B = np.matmul(X,W.T) + b    y = np.argmax(softmax(XW_T_B), axis = 1)    return y'''Problems here ill leave the code though.    if (X.shape[1] != W.shape[0]):        XW_T_B = np.matmul(X,W.T) + b            else:        XW_T_B = np.matmul(X,W) + b'''def softmax(x):    return np.exp(x) / np.sum(np.exp(x), axis= 1, keepdims = True) Y1 = clf.predict(Xtest)Y2 = predict(Xtest,clf.coef_,clf.intercept_)Y = (Y1-Y2)**2print("sum(Y1 - Y2)**2 = {0}".format(np.sum(Y)))#---------------------------------#q1f#---------------------------------print("\nQ1 f)\n")def one_hot(Tint):    Tint = np.array((Tint))    max = Tint.max()+1 # num of classes in Tint    C = np.arange(max)    C = C.reshape((1,C.shape[0])) #turn C into [1,J]    Tint = Tint.reshape((Tint.shape[0],1)) #turn Tint into [N,1]    Thot = Tint == C    Thot = Thot*1    return Thotprint(one_hot(np.array(([4,3,2,3,2,1,2,1,0]))))#---------------------------------#q2a#---------------------------------print("\nQ2)\n")def GDlinear(I,lrate):    ''' Xtrain.shape = 2000*2 = Xtest.shape        Xtrain_update.shape = K*(D+1) = 2000*3 = Xtest_update.shape        Weight matrix with B absorbed: (D+1)*K = 3*2000, why?        you cannot preset this weight matrix shape. Recall that weight matrix        is contigent upon (classes)*(predictors)'''            np.random.seed(7)    print("learning rate = ",lrate)    p = np.ones(Xtrain.shape[0]).reshape((Xtrain.shape[0],1))    q = np.ones(Xtest.shape[0]).reshape((Xtest.shape[0],1))    Xtrain_update = np.hstack((p, Xtrain))    Xtest_update = np.hstack((q, Xtest))    Ttrain_update = one_hot(Ttrain)    Ttest_update = one_hot(Ttest)        training_ce = []    test_ce = []    training_acc = []    test_acc = []        '''So remember that the |W| when the biases are absorbed and using    a column of ones in the predictor. it becomes (d+1)*k, where k here is the    # of output classes. its predicting only 0 or 2 for some reason.'''        weight_b = np.random.randn(3,3)/10000     for i in range(I):        #Gradient descent algorithm        #One instance of predict is needed to calculate the derivative.        Y = predict(Xtrain_update, weight_b, b = 0)        derivative = (np.matmul((one_hot(Y) - Ttrain_update).T,Xtrain_update))        derivative = derivative/2000 #prev line was going off the page...        weight_b = weight_b - lrate*(derivative)            #update the training avg cross entropy loss with the current weights        XW_T_B = np.matmul(Xtrain_update,weight_b)        s = softmax(XW_T_B)        s = np.log(s) #apply log(y)        #conduct -t.T(log(y)) for Ttrain        cross_entropy_Ttrain = np.matmul((-Ttrain_update.T),s) #should be 3x3 matrix        cross_entropy_Ttrain = (np.sum(cross_entropy_Ttrain))/2000        training_ce.append(cross_entropy_Ttrain)        #Training accuracy        q = np.array(([2,1,1]))        q = q.reshape((3,1)) #no change because you didnt subtract from predict fn        o = np.matmul((one_hot(Y) - Ttrain_update),q)        o = o**2        o = 1 - o        accuracy = (np.sum(o))/2000        training_acc.append(accuracy)                #Test CEloss        XW_T_B = np.matmul(Xtest_update,weight_b)        s = softmax(XW_T_B)        s = np.log(s) #apply log(y)        #conduct -t.T(log(y)) for Tt        cross_entropy_Ttest = (np.matmul((-Ttest_update.T),s)) #should be 3x3 matrix        cross_entropy_Ttest = (np.sum(cross_entropy_Ttest))/2000        test_ce.append(cross_entropy_Ttest)                #test accuracy        Y_test = predict(Xtest_update, weight_b, b = 0)        o = np.matmul((one_hot(Y_test) - Ttest_update),q)        o = o**2        o = 1 - o        accuracy = (np.sum(o))/2000        test_acc.append(accuracy)   #q2 vii    plt.semilogx(training_ce)    plt.title("Question 2(a): Training and test loss v.s. iterations")    plt.xlabel("Iteration Number")    plt.ylabel("Cross Entropy")    plt.semilogx(test_ce, color ="red")    plt.show()    #q2 viii    plt.clf()    plt.semilogx(training_acc)    plt.title("Question 2(a): Training and test accuracy v.s. iterations")    plt.xlabel("Iteration Number")    plt.ylabel("Accuracy")    plt.semilogx(test_acc, color ="red")    plt.show()    #q2 ix    plt.clf()    plt.semilogx(test_ce[49:])    plt.title("Question 2(a): test loss from iteration 50 on")    plt.xlabel("Iteration Number")    plt.ylabel("Cross Entropy")    plt.semilogx(test_acc, color ="red")    plt.show()    #q2 x    plt.clf()    plt.semilogx(training_ce[49:])    plt.title("Question 2(a): training loss from iteration 50 on")    plt.xlabel("Iteration Number")    plt.ylabel("Cross Entropy")    plt.semilogx(test_acc, color ="blue")    plt.show()    #q2 xi    print("\nQ2 xi)\n")    print("training acc after gradient descent= {0}".format(training_acc[-1]))    print("q1 training acc = {0}".format(Xtrain_score))    print("train descent acc - q1 train acc = {0}".format((training_acc[-1] - \                                               Xtrain_score)))#q2 xii    print("\nQ2 xii)\n")    print("test acc after gradient descent= {0}".format(test_acc[-1]))    print("q1 test acc = {0}".format(Xtest_score))    print("test descent acc - q1 test acc = {0}".format((test_acc[-1] - \                                               Xtest_score)))        GDlinear(10000,0.01)#---------------------------------#q2d I dont know#---------------------------------print("\nQ2 d: I dont know \n")#---------------------------------#q3a#---------------------------------print("\nQ3 a)\n")clf = QuadraticDiscriminantAnalysis(store_covariance=(True))clf.fit(Xtrain, Ttrain)print("Quad Discrim Anaysis training acc= {0}".format(clf.score(Xtrain,Ttrain)))    print("Quad Discrim Anaysis test acc= {0}".format(clf.score(Xtest,Ttest)))plt.clf()bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(clf)bl2d.plt.suptitle("Question 3(a): decision boundaries for quad discriminate analysis")#---------------------------------#q3b#---------------------------------print("\nQ3 b)\n")gnb = GaussianNB()gnb.fit(Xtrain,Ttrain)print("Naive bayes training acc ={0}".format(gnb.score(Xtrain,Ttrain)))print("Naive bayes test acc ={0}".format(gnb.score(Xtest,Ttest)))plt.clf()bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(gnb)bl2d.plt.suptitle("Question 3(b): Naive Bayes")#---------------------------------#3f#---------------------------------print("\nQ3 f)\n")def EstMean(X,T):    T = one_hot(T)    numerator = np.matmul(T.T,X)    denominator = np.sum(T,axis = 0)    denominator = 1 / denominator    mu = np.multiply(numerator, denominator[:, None])    return muprint("sum(QDA mean matrix - EstMean matrix)^2 = {0}".format\      (np.sum((clf.means_ - EstMean(Xtrain,Ttrain))**2)))#---------------------------------#Rest q3 g-k i dont know#---------------------------------print("\n Q3 g-k I dont know\n")#---------------------------------#q4a#---------------------------------print("\nQ4 a)\n")np.random.seed(7)mlp = MLPClassifier(solver = 'sgd',activation = 'logistic',\                           batch_size=200,learning_rate_init =0.01,\                               max_iter=10000,shuffle = True,                               tol = 1e-6, hidden_layer_sizes = (5))mlp.fit(Xtrain,Ttrain)print("mlp 5 hidden units training acc ={0}".format(mlp.score(Xtrain,Ttrain)))print("mlp 5 hidden units test acc ={0}".format(mlp.score(Xtest,Ttest)))plt.clf()bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(mlp)bl2d.plt.suptitle("Question 4(a): Neural net with 5 hidden units")plt.clf()#---------------------------------#4b#---------------------------------#1 hidden unitsprint("\nQ4 b)\n")np.random.seed(7)mlp1 = MLPClassifier(solver = 'sgd',activation = 'logistic',\                           batch_size=200,learning_rate_init =0.01,\                               max_iter=10000,shuffle = True,                               tol = 1e-6, hidden_layer_sizes = (1))mlp1.fit(Xtrain,Ttrain)print("mlp 1 hidden units training acc ={0}".format(mlp1.score(Xtrain,Ttrain)))print("mlp 1 hidden units test acc ={0}\n".format(mlp1.score(Xtest,Ttest)))#sub plot for 1 hidden unitplt.subplot(2,2,1)bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(mlp1)bl2d.plt.title("1 hidden unit")    #2 hidden unitsnp.random.seed(7)mlp2 = MLPClassifier(solver = 'sgd',activation = 'logistic',\                           batch_size=200,learning_rate_init =0.01,\                               max_iter=10000,shuffle = True,                               tol = 1e-6, hidden_layer_sizes = (2))mlp2.fit(Xtrain,Ttrain)print("mlp 2 hidden units training acc ={0}".format(mlp2.score(Xtrain,Ttrain)))print("mlp 2 hidden units test acc ={0}\n".format(mlp2.score(Xtest,Ttest)))#subplot for 2 hidden unitsplt.subplot(2,2,2)bl2d.plot_data(Xtrain, Ttrain)       bl2d.boundaries(mlp2)bl2d.plt.title("2 hidden units")#4 hidden unitsnp.random.seed(7)mlp4 = MLPClassifier(solver = 'sgd',activation = 'logistic',\                           batch_size=200,learning_rate_init =0.01,\                               max_iter=10000,shuffle = True,                               tol = 1e-6, hidden_layer_sizes = (4))mlp4.fit(Xtrain,Ttrain)print("mlp 4 hidden units training acc ={0}".format(mlp4.score(Xtrain,Ttrain)))print("mlp 4 hidden units test acc ={0}\n".format(mlp4.score(Xtest,Ttest)))    #subplot for 4 hidden unitsplt.subplot(2,2,3)bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(mlp4)bl2d.plt.title("4 hidden units")#10 hidden unitsnp.random.seed(7)mlp10 = MLPClassifier(solver = 'sgd',activation = 'logistic',\                           batch_size=200,learning_rate_init =0.01,\                               max_iter=10000,shuffle = True,                               tol = 1e-6, hidden_layer_sizes = (10))mlp10.fit(Xtrain,Ttrain)print("mlp 10 hidden units training acc ={0}".format(mlp10.score(Xtrain,Ttrain)))print("mlp 10 hidden units test acc ={0}\n".format(mlp10.score(Xtest,Ttest)))#subplot for 10 hidden unitsplt.subplot(2,2,4)bl2d.plot_data(Xtrain, Ttrain)bl2d.boundaries(mlp10)bl2d.plt.title("10 hidden units")bl2d.plt.suptitle("Question 4b) Neural net decision boundries")bl2d.plt.tight_layout()bl2d.plt.show()                                                                                    