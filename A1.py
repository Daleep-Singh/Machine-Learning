import numpy as np
import numpy.random as rnd
import sys
np.set_printoptions(threshold=sys.maxsize)

# Read the data
# Download "data.csv" from the course website
data = np.genfromtxt('data.csv', delimiter=',', skip_header=1)

# Display the *shape* of the data matrix
print(data.shape)

# Please leave these print statements, to help your TAs grade quickly.
print('\n\nQuestion 1')
print('----------')

print('\nQuestion 1(a):') # Please leave print statements like these
print(data[:10,0])

print('\nQuestion 1(b):')
print(data[:10,1])


print('\nQuestion 1(d):')
data = data[:,1:]
print(data.shape)

      
print('\nQuestion 1(f):')
test = data[data[:, 0] > 2013.417] 
inter = data[:, 0] > 2013.417 #turns into array of booleans
#we did not need to use np.all here because the array is flattened. If we
#had a 2d array then youd have to use np.all with np.where
train_valid = np.delete(data, np.where(inter),axis = 0) 
print(test.shape)
print(train_valid.shape)

print('\nQuestion 1(g):')

# Below array was generated by calling
# randarray = rnd.randint(0, 5, train_valid.shape[0])
#above will create a probability distribution of 80% training 20% validation
#exluding the test set. with the test set thats: 66% tr, 16% valid, 16% test
# Do NOT uncomment the above line of code. Instead, we are including
# the values you should use below.
randarray = np.array([2, 0, 1, 3, 0, 0, 0, 3, 2, 3, 1, 1, 2, 0, 4, 4, 0, 2, 1, 2, 2, 2,
                      4, 1, 3, 2, 0, 1, 2, 0, 3, 0, 3, 1, 3, 0, 4, 1, 4, 4, 0, 0, 1, 2,
                      4, 0, 0, 1, 1, 1, 2, 3, 4, 4, 3, 3, 0, 0, 0, 0, 2, 2, 3, 0, 0, 1,
                      4, 1, 4, 2, 2, 4, 4, 2, 0, 4, 0, 3, 2, 0, 4, 3, 1, 1, 0, 0, 0, 0,
                      1, 1, 4, 0, 3, 4, 2, 0, 0, 4, 4, 4, 4, 3, 3, 0, 0, 2, 2, 1, 3, 2,
                      4, 1, 2, 2, 3, 4, 1, 4, 3, 1, 1, 3, 0, 4, 4, 4, 0, 3, 3, 0, 4, 0,
                      0, 4, 3, 4, 1, 2, 2, 4, 4, 1, 2, 1, 1, 0, 4, 4, 4, 2, 0, 4, 2, 0,
                      4, 4, 1, 4, 0, 4, 0, 0, 1, 4, 3, 2, 4, 3, 1, 4, 1, 3, 4, 1, 0, 0,
                      4, 4, 2, 0, 4, 4, 4, 2, 3, 3, 4, 1, 0, 1, 2, 3, 1, 0, 1, 3, 4, 0,
                      0, 1, 2, 2, 2, 2, 4, 3, 1, 1, 4, 4, 1, 4, 2, 4, 0, 2, 4, 1, 3, 0,
                      4, 2, 3, 0, 4, 2, 3, 2, 2, 0, 2, 0, 2, 3, 2, 3, 4, 2, 4, 2, 2, 4,
                      3, 4, 0, 4, 4, 0, 1, 4, 2, 4, 2, 4, 0, 3, 4, 2, 1, 1, 3, 0, 1, 0,
                      3, 1, 3, 2, 4, 3, 1, 3, 0, 3, 0, 4, 2, 1, 2, 3, 2, 2, 4, 1, 4, 2,
                      1, 3, 1, 2, 2, 3, 1, 4, 2, 2, 4, 4, 1, 3, 2, 4, 1, 2, 4, 4, 0, 3,
                      1, 2, 1, 3, 3, 3, 2, 1, 3, 0, 2, 4, 2, 0, 3, 1, 0, 4, 2, 4, 1, 0,
                      4, 2, 2, 0, 1, 4, 3, 4, 0, 3, 2, 0, 2, 0])
train = np.delete(train_valid, np.where(randarray[:] == 0), axis = 0)
valid = np.delete(train_valid, np.where(randarray[:] != 0),axis = 0)

print(train.shape)
print(valid.shape)

print('\nQuestion 1(h):')

train_x = train[:, 1:6] #includes all relevent predictors
train_t = train[:, 6]   #includes the response variable
valid_x = valid[:, 1:6]
valid_t = valid[:, 6]
test_x = test[:, 1:6]
test_t = test[:, 6]

print(train_x[:2]) 
print(train_t[:2]) 
print(valid_x[:2]) 
print(valid_t[:2]) 
print(test_x[:2]) 
print(test_t[:2]) 

print('\nQuestion 1(i):')
data_mean = np.mean(data, axis = 0) #axis = 0 is column wise
data_std = np.std(data, axis = 0)
x_mean = np.mean(train_x, axis = 0)
x_std = np.std(train_x,axis = 0)

print("data mean = {0}\n".format(data_mean))
print("data std = {0}\n".format(data_std))
print("train_x mean = {0}\n".format(x_mean))
print("train_x std ={0}\n".format(x_std))

print('\nQuestion 1(m):')
#broadcasting is used because x_mean.shape = (5,) =x_std.shape
#and train_x.shape = (273,5), so the former must be broadcast.
norm_train_x = (train_x - (x_mean)) / (x_std)
print("mean of columns of norm_train_x = {0}\n".format(np.mean(norm_train_x, axis = 0)))
print("first two rows of norm_train_x = {0}".format(norm_train_x[:2,]))

print('\nQuestion 1(n):')
import time
nonvec_before = time.time()

norm_train_x_loop = np.zeros_like(train_x)
for i in range(train_x.shape[0]):
    for j in range(train_x.shape[1]):
        norm_train_x_loop = (train_x[i, j] - x_mean[j]) / x_std[j]

nonvec_after = time.time()
print("Non-vectorized time: ", nonvec_after - nonvec_before)


vec_before = time.time()
norm_train_x = (train_x - (x_mean)) / (x_std)
vec_after = time.time()
print("Vectorized time: ", vec_after - vec_before)

# Include your response in your writeup

# Please leave these print statements, to help your TAs grade quickly.
print('\n\nQuestion 2')
print('----------')
print('\nQuestion 2(a):')

v = valid_x[0] # should be np.array([ 19.5    , 306.5947 ,   9.     ,  24.98034, 121.53951])

distances = ((train_x - v)**2).sum(axis = 1)
print("10 values of distance matrix:\n")
print(distances[:10])
print("\n")
n = np.argmin(distances)
print("n = 0")

print("train_x[n]:{0}".format(train_x[n]))

print("train_t[n]: {0}".format(train_t[n]))

print('\nQuestion 2(b):')
s = distances.argsort()
where = np.where(s < 4)

knn_3_prediction = (np.sum(train_t[where]))/3
print("3knn prediction: {0}".format(knn_3_prediction))

print('\nQuestion 2(c):')
def unnorm_knn(v, k, features=train_x, labels=train_t):
    """
    Returns the k Nearest Neighbour prediction of housing prices for an input
    vector v.

    Parameters:
        v - The input vector to make predictions for
        k - The hyperparameter "k" in kNN
        features - The input features of the training data; a numpy array of shape [N, D]
                   (By default, `train_x` is used)
        labels - The target labels of the training data; a numpy array of shape [N]
                 (By default, `train_t` is used)
    """
    distances = ((features - v)**2).sum(axis = 1)
    s = distances.argsort()
    where = np.where(s < k)
    knn_k_prediction = (np.sum(labels[where]))/k
    return knn_k_prediction

print(unnorm_knn(v=valid_x[1], k=5))

print('\nQuestion 2(d):')
def compute_mse(predict, data_x=valid_x, data_t=valid_t):
    """
    Returns the Mean Squared Error of a model across a dataset

    Parameters:
        predict - A Python *function* that takes an input vector and produces a
                  prediction for that vector.
        data_x - The input features of the data set to make predictions for
                 (By default, `valid_x` is used)
        data_t - The target labels of the dataset to make predictions for 
                 (By default, `train_t` is used)
    """

    errors = []
    for i in range(data_t.shape[0]):
        error = (predict(data_x[i]) - data_t[i])**2
        errors.append(error) 
    return np.mean(errors)

def baseline(v):
    """
    Returns the average housing price given an input vector v.
    """
    return np.mean(train_t)

# compute and print the training and validation MSE
print(compute_mse(baseline, data_x=train_x, data_t=train_t))
print(compute_mse(baseline, data_x=valid_x, data_t=valid_t))

print('\nQuestion 2(e):')

train_mse = []
valid_mse = []
for k in range(1, 31):
    # create a temporary function `predict_fn` that computes the knn
    # prediction for the current value of the loop variable `k`
    def predict_fn(new_v):
        return unnorm_knn(new_v, k) #returns a singular numeric
    
    # compute the training and validation MSE for this kNN model
    mse = compute_mse(predict_fn, data_x=train_x, data_t=train_t)
    train_mse.append(mse)
    mse = compute_mse(predict_fn, data_x=valid_x, data_t=valid_t)
    valid_mse.append(mse)


from matplotlib import pyplot as plt
plt.plot(range(1, 31), train_mse)
plt.plot(range(1, 31), valid_mse)
plt.xlabel("k")
plt.ylabel("MSE")
plt.title("Unnormalized kNN")
plt.legend(["Training", "Validation"])
plt.show()

# Include your response in your writeup

print('\nQuestion 2(f):')

def norm_knn(v, k, features=norm_train_x, means=x_mean, stds=x_std, labels=train_t):
    """
    Returns the k Nearest Neighbour prediction of housing prices for an input
    vector v.

    Parameters:
        v - The input vector to make predictions for
        k - The hyperparameter "k" in kNN
        features - The normalized input features of the training data
                   (By default, `norm_train_x` is used)
        means - The means over the training data (By default, `x_mean` is used)
        stds - The standard deviations of the training data (By default, `x_std` is used)
        labels - The target labels of the training data; a numpy array of shape [N]
                 (By default, `train_t` is used)
    """
    v = (v - means)/stds #normalize input vector
    distances = ((features - v)**2).sum(axis = 1)
    s = distances.argsort()
    where = np.where(s < k)
    knn_k_prediction = (np.sum(labels[where]))/k
    return knn_k_prediction

train_mse = []
valid_mse = []
for k in range(1, 31):
    # create a temporary function `predict_fn` that computes the knn
    # prediction for the current value of the loop variable `k`
    def predict_fn(new_v):
        return norm_knn(new_v, k)

    # compute the training and validation MSE for this kNN model
    mse = compute_mse(predict_fn, data_x=train_x, data_t=train_t)
    train_mse.append(mse)
    mse = compute_mse(predict_fn, data_x=valid_x, data_t=valid_t)
    valid_mse.append(mse)


from matplotlib import pyplot as plt
plt.plot(range(1, 31), train_mse)
plt.plot(range(1, 31), valid_mse)
plt.xlabel("k")
plt.ylabel("MSE")
plt.title("Normalized kNN")
plt.legend(["Training", "Validation"])
plt.show()

print('\n\nQuestion 3')
print('----------')
train_d = train_x[:, 0] # house age
# Plot this feature against the target (house price)
plt.scatter(train_d, train_t)
plt.xlabel("House Age")
plt.ylabel("House Price")
plt.title("House Age vs. Price")
plt.show()

print('\nQuestion 3(a):')
X = np.stack((np.ones(273),train_d),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
linear_coef = np.matmul(X_T_X_inv, X_T_T)
print("linear_coef = {0}".format(linear_coef))

#3b
def pred_linear(v, coef=linear_coef):
    """
    Returns the linear regression predictions of house prices, given
    9
    a vector consisting of the ages of several houses that we would
    like to make predictions for.
    Parameters:
    v - A vector of house ages
    coef - The linear regression coefficient
    """
    y = (v*linear_coef[1]) + linear_coef[0]
    return y

def plot_prediction(predict, title):
    """
    Display a plot that superimposes the model predictions on a scatter
    plot of the training data (train_d, train_t)
    Parameters:
    predict - A Python *function* that takes an input vector and produces a
    prediction for that vector.
    title - A title to display on the figure.
    """
    # start with a scatter plot
    plt.scatter(train_d, train_t)
    # create several "house age" values to make predictions for
    min_age = np.min(train_d)
    max_age = np.max(train_d)
    v = np.arange(min_age, max_age, 0.1)
    # make predictions for those values
    y = predict(v)
    # plot the result
    plt.plot(v, y)
    plt.title(title)
    plt.xlabel("House Age")
    plt.ylabel("House Price")
    plt.show()
    
plot_prediction(pred_linear, title="Linear Regression (no feature expansion)")

#3c
def compute_mse_vectorized(predict, data_x=valid_x, data_t=valid_t):
    """
    Returns the Mean Squared Error of a model across a dataset
    Parameters:
    predict - A Python *function* that takes a vector of house ages,
    and produces a vector of house price predictions
    data_x - The input features of the data set to make predictions for
    (By default, `valid_x` is used)
    10
    data_t - The target labels of the dataset to make predictions for
    (By default, `train_t` is used)
    """
    v = data_x[:, 0] # house age
    y = predict(v) #prediction
    return np.sum((y-data_t)**2)/v.shape

print('\nQuestion 3(d):')
X = np.stack((np.ones(273),train_d,(train_d)**2),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
quad_coef = np.matmul(X_T_X_inv, X_T_T)

print("quad_coef {0}".format(quad_coef))

#3e
def pred_quad(v, coef=quad_coef): 
    """
    Returns the degree 2 polynomial regression predictions of
    house prices, given a vector consisting of the ages of several houses
    that we would like to make predictions for.
    Parameters:
        v - A vector of house ages
        coef - The linear regression coefficient
    """
    
    y = v*coef[1] + (v**2)*coef[2] + coef[0]
    return (y)

plot_prediction(pred_quad, title="Polynomial Regression (M=2)")

print('\nQuestion 3(f):')
print("training error: {0}".format(compute_mse_vectorized(pred_quad,\
                                                          train_x, train_t)))
print("validation error: {0}".format(compute_mse_vectorized(pred_quad,\
                                                            valid_x, valid_t)))

print('\nQuestion 3(g):')
lst = [np.ones(273),train_d,train_d**2,train_d**3,train_d**4,train_d**5,train_d**6,\
       train_d**7,train_d**8,train_d**9,train_d**10]


X = np.stack((lst[:4]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
three_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_3(v,coef=three_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + coef[0]
    return (y)

X = np.stack((lst[:5]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
four_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_4(v,coef=four_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4]+ coef[0]
    return (y)

X = np.stack((lst[:6]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
five_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_5(v,coef=five_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
        (v**5)*coef[5] + coef[0]
    return (y)

X = np.stack((lst[:7]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
six_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_6(v,coef=six_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
    (v**5)*coef[5] + (v**6)*coef[6]+ coef[0]
    return (y)

X = np.stack((lst[:8]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
seven_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_7(v,coef=seven_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
        (v**5)*coef[5] + (v**6)*coef[6] + (v**7)*coef[7]+ coef[0]
    return (y)

X = np.stack((lst[:9]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
eight_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_8(v,coef=eight_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
        (v**5)*coef[5] + (v**6)*coef[6] + (v**7)*coef[7]+ (v**8)*coef[8] + \
            coef[0]
    return (y)

X = np.stack((lst[:10]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
nine_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_9(v,coef=nine_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
        (v**5)*coef[5] + (v**6)*coef[6] + (v**7)*coef[7]+ (v**8)*coef[8] + \
            (v**9)*coef[9] + coef[0]
    return (y)

X = np.stack((lst[:11]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
ten_coef = np.matmul(X_T_X_inv, X_T_T)
def pred_10(v,coef=ten_coef):
    y = v*coef[1] + (v**2)*coef[2] + (v**3)*coef[3] + (v**4)*coef[4] + \
        (v**5)*coef[5] + (v**6)*coef[6] + (v**7)*coef[7]+ (v**8)*coef[8] + \
            (v**9)*coef[9] + (v**10)*coef[10]+ coef[0]
    return (y)

train_mse_r = [] 
valid_mse_r = []

#adding to train_mse_r
train_mse_r.append(compute_mse_vectorized(baseline,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_linear,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_quad,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_3,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_4,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_5,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_6,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_7,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_8,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_9,train_x,train_t))
train_mse_r.append(compute_mse_vectorized(pred_10,train_x,train_t))


#adding to valid_mse_r
valid_mse_r.append(compute_mse_vectorized(baseline,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_linear,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_quad,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_3,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_4,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_5,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_6,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_7,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_8,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_9,valid_x,valid_t))
valid_mse_r.append(compute_mse_vectorized(pred_10,valid_x,valid_t))
 
 
print(train_mse_r)
print(valid_mse_r)

#3i

plt.plot(range(0, 11), train_mse_r)
plt.plot(range(0, 11), valid_mse_r)
plt.xlabel("Polynomial Degree")
plt.ylabel("MSE")
plt.title("Polynomial Regression")
plt.legend(["Training", "Validation"])
plt.show()

#3j

plot_prediction(pred_3,title="Polynomial Regression (M= 3)")
plot_prediction(pred_4,title="Polynomial Regression (M= 4)")
plot_prediction(pred_5,title="Polynomial Regression (M= 5)")
plot_prediction(pred_6,title="Polynomial Regression (M= 6)")
plot_prediction(pred_7,title="Polynomial Regression (M= 7)")
plot_prediction(pred_8,title="Polynomial Regression (M= 8)")
plot_prediction(pred_9,title="Polynomial Regression (M= 9)")
plot_prediction(pred_10,title="Polynomial Regression (M= 10)")

#3k
'''
X = np.stack((np.ones(273),norm_train_x[:,0]),axis = 1)
X_T_X = np.matmul(X.T,X)
X_T_X_inv = np.linalg.inv(X_T_X)
X_T_T = np.matmul(X.T,train_t)
linear_coef_norm = np.matmul(X_T_X_inv, X_T_T)

print(compute_mse_vectorized(pred_linear))
print(compute_mse_vectorized(pred_linear(coef = linear_coef_norm),norm_train_x,train_t))
'''

print('\n\nQuestion 4')
print('----------')
print('\nQuestion 4(e):')
def grad(weight, X, t): 
    '''
    Return gradient of each weight evaluated at the current value
        Parameters:
    `weight` - a current "guess" of what our weights should be,
                   a numpy array of shape (D)
    `X` - matrix of shape (N,D) of input features
    `t` - target y values of shape (N)
    '''
    return np.matmul(((np.matmul(X,weight) - t)*(1/X.shape[0])),X)



# Please leave this print statement for grading:
print(grad(np.array([1]), np.array([[1], [1]]), np.array([2, 2])))


